# services/rag.py
from openai import OpenAI
import os
import json
import ast
import numpy as np
from sqlalchemy.orm import Session
from models import PDFChunk

# Ú©Ù„Ø§ÛŒÙ†Øª Ø¬Ø¯ÛŒØ¯ OpenAI Ø¨Ø§ SDK Ù‡Ø§ÛŒ >=1.0.0
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def get_embedding(text: str) -> list:
    """
    Ú¯Ø±ÙØªÙ† Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ Ø¨Ø§ Ù…Ø¯Ù„ text-embedding-3-small
    """
    resp = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return resp.data[0].embedding

def store_chunks(chunks: list, document_name: str, db: Session, user_id: int):
    """
    Ø°Ø®ÛŒØ±Ù‡â€ŒÛŒ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ ÛŒÚ© PDF Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ Ø´Ù†Ø§Ø³Ù‡â€ŒÛŒ Ú©Ø§Ø±Ø¨Ø±.
    """
    for chunk in chunks:
        embedding = get_embedding(chunk)
        chunk_record = PDFChunk(
            filename=document_name,
            chunk=chunk,
            embedding=embedding,   # Ø³ØªÙˆÙ† JSON ÛŒØ§ Ù…ØªÙ†Ø› SQLAlchemy Ù‡Ù†Ø¯Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
            user_id=user_id
        )
        db.add(chunk_record)
    db.commit()

def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    """
    Ø´Ø¨Ø§Ù‡Øª Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ Ø¨ÛŒÙ† Ø¯Ùˆ Ø¨Ø±Ø¯Ø§Ø±
    """
    denom = (np.linalg.norm(a) * np.linalg.norm(b))
    if denom == 0:
        return 0.0
    return float(np.dot(a, b) / denom)

def _to_numpy_embedding(value) -> np.ndarray:
    """
    ÙˆØ±ÙˆØ¯ÛŒ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ Ø±Ø§ (Ù„ÛŒØ³Øª/Ø±Ø´ØªÙ‡) Ø¨Ù‡ numpy.ndarray (float) ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    if value is None:
        return np.array([], dtype=float)
    if isinstance(value, (list, tuple)):
        return np.array(value, dtype=float)
    if isinstance(value, str):
        # Ø§Ú¯Ø± Ø¨Ù‡ ØµÙˆØ±Øª Ø±Ø´ØªÙ‡ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯Ø› Ø§ÙˆÙ„ ØªÙ„Ø§Ø´ JSONØŒ Ø¨Ø¹Ø¯ literal_eval Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ
        try:
            parsed = json.loads(value)
        except json.JSONDecodeError:
            parsed = ast.literal_eval(value)
        return np.array(parsed, dtype=float)
    # Ù‡Ø± Ø­Ø§Ù„Øª Ø¯ÛŒÚ¯Ø±:
    return np.array(value, dtype=float)

def search_similar_chunks(query: str, db: Session, filename: str, top_k: int = 3) -> list:
    """
    Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ Ù¾Ø±Ø³Ø´ØŒ Ù…Ø´Ø§Ø¨Ù‡â€ŒØªØ±ÛŒÙ† Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    """
    query_embedding = get_embedding(query)
    query_embedding_np = np.array(query_embedding, dtype=float)

    chunks = db.query(PDFChunk).filter(PDFChunk.filename == filename).all()
    results = []

    for chunk in chunks:
        chunk_embedding_np = _to_numpy_embedding(chunk.embedding)
        if chunk_embedding_np.size == 0:
            continue
        score = cosine_similarity(query_embedding_np, chunk_embedding_np)
        results.append((score, chunk.chunk))

    results.sort(reverse=True, key=lambda x: x[0])
    return [text for _, text in results[:top_k]]

def delete_chunks_by_filename(filename: str, db: Session):
    db.query(PDFChunk).filter(PDFChunk.filename == filename).delete()
    db.commit()

def get_chunks_by_filename(filename: str, db: Session):
    return db.query(PDFChunk).filter(PDFChunk.filename == filename).all()

def answer_question(filename: str, query: str, db: Session) -> str:
    """
    Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³Ø¤Ø§Ù„ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙ…Ø§Ù… Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ ÙØ§ÛŒÙ„ (Ø¨Ø¯ÙˆÙ† Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ).
    """
    chunks = get_chunks_by_filename(filename, db)
    if not chunks:
        return "Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯."

    context_chunks = [str(chunk.chunk) for chunk in chunks]
    context = "\n\n".join(context_chunks)

    prompt = f"""Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²ÛŒØ± Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡:

{context}

Ø³ÙˆØ§Ù„: {query}
Ù¾Ø§Ø³Ø®:"""

    resp = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    content = resp.choices[0].message.content
    if content is None:
        return "Ù¾Ø§Ø³Ø®ÛŒ Ø§Ø² Ù…Ø¯Ù„ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯."
    return content.strip()

def generate_answer(query: str, context_chunks: list) -> str:
    """
    Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³Ø¤Ø§Ù„ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ (Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒØ´Ø¯Ù‡ Ø¨ÛŒØ±ÙˆÙ† Ø§Ø² Ø§ÛŒÙ† ØªØ§Ø¨Ø¹).
    """
    context_chunks_str = [str(chunk) for chunk in context_chunks if chunk is not None]
    context = "\n\n".join(context_chunks_str)

    prompt = f"""Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²ÛŒØ± Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡:

{context}

Ø³ÙˆØ§Ù„: {query}
Ù¾Ø§Ø³Ø®:"""

    print("ğŸ“¥ Prompt to OpenAI:", prompt[:500])

    resp = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    print("ğŸ“¤ Raw response:", resp)

    content = resp.choices[0].message.content
    return content.strip() if content else "â— Ù¾Ø§Ø³Ø® Ø®Ø§Ù„ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯."
